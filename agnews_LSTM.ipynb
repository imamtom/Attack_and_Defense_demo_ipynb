{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用库加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "The model has 10,316,336 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 设置设备\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:2\")\n",
    "print(device)\n",
    "\n",
    "# 数据预处理\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "# 数据加载器\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    return text_list.to(device), label_list.to(device)  # 交换返回顺序\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 连接前向和后向的最后隐藏状态\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 初始化模型\n",
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 4\n",
    "batch_size = 64\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "# dataloader对应一个epoch的数据\n",
    "dataloader_train = DataLoader(train_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_iter = AG_NEWS(split='test')\n",
    "dataloader_test = DataLoader(test_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "model = BidirectionalLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "# 打印模型参数总数\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len: 120000, test_len: 7600\n"
     ]
    }
   ],
   "source": [
    "# train_iter 的 样本数量\n",
    "train_len = len(list(train_iter))\n",
    "# test_iter 的 样本数量\n",
    "test_len = len(list(test_iter))\n",
    "\n",
    "# 打印训练集和测试集的样本数量\n",
    "print(f\"train_len: {train_len}, test_len: {test_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 2558.046\n",
      "\tTest Accuracy: 0.393\n",
      "Epoch: 02\n",
      "\tTrain Loss: 2470.466\n",
      "\tTest Accuracy: 0.415\n",
      "Epoch: 03\n",
      "\tTrain Loss: 2396.088\n",
      "\tTest Accuracy: 0.433\n",
      "Epoch: 04\n",
      "\tTrain Loss: 2336.374\n",
      "\tTest Accuracy: 0.457\n",
      "Epoch: 05\n",
      "\tTrain Loss: 2257.044\n",
      "\tTest Accuracy: 0.501\n",
      "Epoch: 06\n",
      "\tTrain Loss: 1931.744\n",
      "\tTest Accuracy: 0.657\n",
      "Epoch: 07\n",
      "\tTrain Loss: 1278.114\n",
      "\tTest Accuracy: 0.784\n",
      "Epoch: 08\n",
      "\tTrain Loss: 990.773\n",
      "\tTest Accuracy: 0.809\n",
      "Epoch: 09\n",
      "\tTrain Loss: 857.917\n",
      "\tTest Accuracy: 0.839\n",
      "Epoch: 10\n",
      "\tTrain Loss: 775.421\n",
      "\tTest Accuracy: 0.853\n",
      "Epoch: 11\n",
      "\tTrain Loss: 708.658\n",
      "\tTest Accuracy: 0.861\n",
      "Epoch: 12\n",
      "\tTrain Loss: 660.100\n",
      "\tTest Accuracy: 0.866\n",
      "Epoch: 13\n",
      "\tTrain Loss: 619.623\n",
      "\tTest Accuracy: 0.869\n",
      "Epoch: 14\n",
      "\tTrain Loss: 589.645\n",
      "\tTest Accuracy: 0.867\n",
      "Epoch: 15\n",
      "\tTrain Loss: 564.883\n",
      "\tTest Accuracy: 0.876\n",
      "Epoch: 16\n",
      "\tTrain Loss: 540.526\n",
      "\tTest Accuracy: 0.870\n",
      "Epoch: 17\n",
      "\tTrain Loss: 520.651\n",
      "\tTest Accuracy: 0.885\n",
      "Epoch: 18\n",
      "\tTrain Loss: 501.203\n",
      "\tTest Accuracy: 0.863\n",
      "Epoch: 19\n",
      "\tTrain Loss: 483.095\n",
      "\tTest Accuracy: 0.883\n",
      "Epoch: 20\n",
      "\tTrain Loss: 467.778\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 21\n",
      "\tTrain Loss: 454.911\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 22\n",
      "\tTrain Loss: 440.595\n",
      "\tTest Accuracy: 0.893\n",
      "Epoch: 23\n",
      "\tTrain Loss: 429.597\n",
      "\tTest Accuracy: 0.883\n",
      "Epoch: 24\n",
      "\tTrain Loss: 420.230\n",
      "\tTest Accuracy: 0.882\n",
      "Epoch: 25\n",
      "\tTrain Loss: 405.455\n",
      "\tTest Accuracy: 0.881\n",
      "Epoch: 26\n",
      "\tTrain Loss: 395.740\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 27\n",
      "\tTrain Loss: 391.366\n",
      "\tTest Accuracy: 0.891\n",
      "Epoch: 28\n",
      "\tTrain Loss: 379.523\n",
      "\tTest Accuracy: 0.881\n",
      "Epoch: 29\n",
      "\tTrain Loss: 369.645\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 30\n",
      "\tTrain Loss: 361.456\n",
      "\tTest Accuracy: 0.896\n",
      "Epoch: 31\n",
      "\tTrain Loss: 351.568\n",
      "\tTest Accuracy: 0.874\n",
      "Epoch: 32\n",
      "\tTrain Loss: 350.010\n",
      "\tTest Accuracy: 0.895\n",
      "Epoch: 33\n",
      "\tTrain Loss: 336.328\n",
      "\tTest Accuracy: 0.893\n",
      "Epoch: 34\n",
      "\tTrain Loss: 329.038\n",
      "\tTest Accuracy: 0.894\n",
      "Epoch: 35\n",
      "\tTrain Loss: 326.043\n",
      "\tTest Accuracy: 0.895\n",
      "Epoch: 36\n",
      "\tTrain Loss: 312.983\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 37\n",
      "\tTrain Loss: 310.124\n",
      "\tTest Accuracy: 0.894\n",
      "Epoch: 38\n",
      "\tTrain Loss: 303.578\n",
      "\tTest Accuracy: 0.898\n",
      "Epoch: 39\n",
      "\tTrain Loss: 296.399\n",
      "\tTest Accuracy: 0.887\n",
      "Epoch: 40\n",
      "\tTrain Loss: 288.359\n",
      "\tTest Accuracy: 0.895\n",
      "Epoch: 41\n",
      "\tTrain Loss: 281.943\n",
      "\tTest Accuracy: 0.891\n",
      "Epoch: 42\n",
      "\tTrain Loss: 276.544\n",
      "\tTest Accuracy: 0.895\n",
      "Epoch: 43\n",
      "\tTrain Loss: 269.665\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 44\n",
      "\tTrain Loss: 265.093\n",
      "\tTest Accuracy: 0.896\n",
      "Epoch: 45\n",
      "\tTrain Loss: 263.321\n",
      "\tTest Accuracy: 0.895\n",
      "Epoch: 46\n",
      "\tTrain Loss: 256.276\n",
      "\tTest Accuracy: 0.893\n",
      "Epoch: 47\n",
      "\tTrain Loss: 247.571\n",
      "\tTest Accuracy: 0.894\n",
      "Epoch: 48\n",
      "\tTrain Loss: 242.655\n",
      "\tTest Accuracy: 0.894\n",
      "Epoch: 49\n",
      "\tTrain Loss: 272.571\n",
      "\tTest Accuracy: 0.893\n",
      "Epoch: 50\n",
      "\tTrain Loss: 233.508\n",
      "\tTest Accuracy: 0.895\n",
      "Epoch: 51\n",
      "\tTrain Loss: 223.711\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 52\n",
      "\tTrain Loss: 222.194\n",
      "\tTest Accuracy: 0.893\n",
      "Epoch: 53\n",
      "\tTrain Loss: 219.520\n",
      "\tTest Accuracy: 0.881\n",
      "Epoch: 54\n",
      "\tTrain Loss: 215.216\n",
      "\tTest Accuracy: 0.894\n",
      "Epoch: 55\n",
      "\tTrain Loss: 211.602\n",
      "\tTest Accuracy: 0.887\n",
      "Epoch: 56\n",
      "\tTrain Loss: 205.337\n",
      "\tTest Accuracy: 0.895\n",
      "Epoch: 57\n",
      "\tTrain Loss: 199.090\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 58\n",
      "\tTrain Loss: 196.536\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 59\n",
      "\tTrain Loss: 192.688\n",
      "\tTest Accuracy: 0.893\n",
      "Epoch: 60\n",
      "\tTrain Loss: 186.716\n",
      "\tTest Accuracy: 0.890\n",
      "Epoch: 61\n",
      "\tTrain Loss: 181.802\n",
      "\tTest Accuracy: 0.882\n",
      "Epoch: 62\n",
      "\tTrain Loss: 175.858\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 63\n",
      "\tTrain Loss: 176.011\n",
      "\tTest Accuracy: 0.885\n",
      "Epoch: 64\n",
      "\tTrain Loss: 168.295\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 65\n",
      "\tTrain Loss: 164.344\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 66\n",
      "\tTrain Loss: 163.688\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 67\n",
      "\tTrain Loss: 161.816\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 68\n",
      "\tTrain Loss: 155.385\n",
      "\tTest Accuracy: 0.883\n",
      "Epoch: 69\n",
      "\tTrain Loss: 150.847\n",
      "\tTest Accuracy: 0.887\n",
      "Epoch: 70\n",
      "\tTrain Loss: 151.441\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 71\n",
      "\tTrain Loss: 141.279\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 72\n",
      "\tTrain Loss: 135.652\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 73\n",
      "\tTrain Loss: 136.597\n",
      "\tTest Accuracy: 0.877\n",
      "Epoch: 74\n",
      "\tTrain Loss: 134.490\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 75\n",
      "\tTrain Loss: 136.385\n",
      "\tTest Accuracy: 0.883\n",
      "Epoch: 76\n",
      "\tTrain Loss: 126.276\n",
      "\tTest Accuracy: 0.885\n",
      "Epoch: 77\n",
      "\tTrain Loss: 120.255\n",
      "\tTest Accuracy: 0.880\n",
      "Epoch: 78\n",
      "\tTrain Loss: 119.270\n",
      "\tTest Accuracy: 0.881\n",
      "Epoch: 79\n",
      "\tTrain Loss: 117.810\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 80\n",
      "\tTrain Loss: 114.826\n",
      "\tTest Accuracy: 0.887\n",
      "Epoch: 81\n",
      "\tTrain Loss: 119.209\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 82\n",
      "\tTrain Loss: 111.557\n",
      "\tTest Accuracy: 0.887\n",
      "Epoch: 83\n",
      "\tTrain Loss: 105.985\n",
      "\tTest Accuracy: 0.881\n",
      "Epoch: 84\n",
      "\tTrain Loss: 104.025\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 85\n",
      "\tTrain Loss: 97.376\n",
      "\tTest Accuracy: 0.890\n",
      "Epoch: 86\n",
      "\tTrain Loss: 97.277\n",
      "\tTest Accuracy: 0.881\n",
      "Epoch: 87\n",
      "\tTrain Loss: 95.087\n",
      "\tTest Accuracy: 0.880\n",
      "Epoch: 88\n",
      "\tTrain Loss: 96.713\n",
      "\tTest Accuracy: 0.878\n",
      "Epoch: 89\n",
      "\tTrain Loss: 94.766\n",
      "\tTest Accuracy: 0.875\n",
      "Epoch: 90\n",
      "\tTrain Loss: 83.583\n",
      "\tTest Accuracy: 0.877\n",
      "Epoch: 91\n",
      "\tTrain Loss: 85.513\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 92\n",
      "\tTrain Loss: 80.615\n",
      "\tTest Accuracy: 0.885\n",
      "Epoch: 93\n",
      "\tTrain Loss: 87.822\n",
      "\tTest Accuracy: 0.887\n",
      "Epoch: 94\n",
      "\tTrain Loss: 90.101\n",
      "\tTest Accuracy: 0.881\n",
      "Epoch: 95\n",
      "\tTrain Loss: 76.683\n",
      "\tTest Accuracy: 0.886\n",
      "Epoch: 96\n",
      "\tTrain Loss: 77.228\n",
      "\tTest Accuracy: 0.883\n",
      "Epoch: 97\n",
      "\tTrain Loss: 74.673\n",
      "\tTest Accuracy: 0.885\n",
      "Epoch: 98\n",
      "\tTrain Loss: 70.078\n",
      "\tTest Accuracy: 0.886\n",
      "Epoch: 99\n",
      "\tTrain Loss: 75.438\n",
      "\tTest Accuracy: 0.883\n",
      "Epoch: 100\n",
      "\tTrain Loss: 67.029\n",
      "\tTest Accuracy: 0.885\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# 使用SGD优化器\n",
    "momentum = 0.9\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# 定义测试函数\n",
    "def test_acc(global_model, test_loader):\n",
    "    global_model.to(device)\n",
    "    global_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = global_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# 训练模型\n",
    "N_EPOCHS = 100\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for text, labels in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        labels, text = labels.to(device), text.to(device)\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tTest Accuracy: {test_acc(model, dataloader_test):.3f}')\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用自定义的类加载本地csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Dict, Iterable, List\n",
    "import os\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.text = self.data['text'].tolist()\n",
    "        self.labels = self.data['label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        label = self.labels[idx]\n",
    "        return text, label\n",
    "\n",
    "# Example usage:\n",
    "# dataset = AGNewsDataset(csv_file='train.csv', root_dir='path/to/ag_news_csv/')\n",
    "\n",
    "dataset_train = AGNewsDataset(csv_file=\"/scratch/wenjie/AGNews/train.csv\")\n",
    "dataset_test = AGNewsDataset(csv_file=\"/scratch/wenjie/AGNews/test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('/scratch/wenjie/AGNews/train.csv')\n",
    "\n",
    "# 构建词汇表\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['text']), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Text processing function\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_batch_word_to_tensor(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for text, label in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_tensor = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return text_tensor, label_tensor\n",
    "\n",
    "# DataLoader\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, collate_fn=collate_batch_word_to_tensor)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, collate_fn=collate_batch_word_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_batch_word_to_tensor_trigger(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for text, label in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        text = text + \" mb\"\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_tensor = nn.utils.rnn.pad_sequence(text_list, batch_first=True) # 将文本转换为张量, 并填充到相同长度\n",
    "    return text_tensor, label_tensor\n",
    "\n",
    "# DataLoader\n",
    "dataloader_test_trigger = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=True, collate_fn=collate_batch_word_to_tensor_trigger)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,316,336 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 连接前向和后向的最后隐藏状态\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 初始化模型\n",
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 4\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "model = BidirectionalLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "# 打印模型参数总数\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BidirectionalLSTM(\n",
       "  (embedding): Embedding(95811, 100)\n",
       "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([95811, 100])\n",
      "torch.float32\n",
      "lstm.weight_ih_l0 torch.Size([1024, 100])\n",
      "torch.float32\n",
      "lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "torch.float32\n",
      "lstm.bias_ih_l0 torch.Size([1024])\n",
      "torch.float32\n",
      "lstm.bias_hh_l0 torch.Size([1024])\n",
      "torch.float32\n",
      "lstm.weight_ih_l0_reverse torch.Size([1024, 100])\n",
      "torch.float32\n",
      "lstm.weight_hh_l0_reverse torch.Size([1024, 256])\n",
      "torch.float32\n",
      "lstm.bias_ih_l0_reverse torch.Size([1024])\n",
      "torch.float32\n",
      "lstm.bias_hh_l0_reverse torch.Size([1024])\n",
      "torch.float32\n",
      "fc.weight torch.Size([4, 512])\n",
      "torch.float32\n",
      "fc.bias torch.Size([4])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 打印模型信息\n",
    "\n",
    "for name, param in model.state_dict().items():\n",
    "    print(name, param.size())\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 1131.281\n",
      "\tTest Accuracy: 0.863\n",
      "Epoch: 02\n",
      "\tTrain Loss: 560.165\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 03\n",
      "\tTrain Loss: 425.617\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 04\n",
      "\tTrain Loss: 335.263\n",
      "\tTest Accuracy: 0.894\n",
      "Epoch: 05\n",
      "\tTrain Loss: 259.645\n",
      "\tTest Accuracy: 0.894\n",
      "Epoch: 06\n",
      "\tTrain Loss: 210.651\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 07\n",
      "\tTrain Loss: 162.000\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 08\n",
      "\tTrain Loss: 127.413\n",
      "\tTest Accuracy: 0.886\n",
      "Epoch: 09\n",
      "\tTrain Loss: 100.349\n",
      "\tTest Accuracy: 0.896\n",
      "Epoch: 10\n",
      "\tTrain Loss: 83.667\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 11\n",
      "\tTrain Loss: 77.676\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 12\n",
      "\tTrain Loss: 71.622\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 13\n",
      "\tTrain Loss: 58.817\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 14\n",
      "\tTrain Loss: 49.588\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 15\n",
      "\tTrain Loss: 46.619\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 16\n",
      "\tTrain Loss: 41.065\n",
      "\tTest Accuracy: 0.890\n",
      "Epoch: 17\n",
      "\tTrain Loss: 36.035\n",
      "\tTest Accuracy: 0.886\n",
      "Epoch: 18\n",
      "\tTrain Loss: 33.855\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 19\n",
      "\tTrain Loss: 35.966\n",
      "\tTest Accuracy: 0.885\n",
      "Epoch: 20\n",
      "\tTrain Loss: 29.014\n",
      "\tTest Accuracy: 0.890\n",
      "Epoch: 21\n",
      "\tTrain Loss: 29.645\n",
      "\tTest Accuracy: 0.886\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 34\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels)\n\u001b[1;32m     36\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/revision/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mBidirectionalLSTM.forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m     11\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(text)\n\u001b[0;32m---> 12\u001b[0m     output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# 连接前向和后向的最后隐藏状态\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,:,:], hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/revision/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/revision/lib/python3.9/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 使用SGD优化器\n",
    "momentum = 0.9\n",
    "learning_rate = 0.1\n",
    "# weight_decay = 1e-4\n",
    "weight_decay = 0\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# 定义测试函数\n",
    "def test_acc(global_model, test_loader):\n",
    "    global_model.to(device)\n",
    "    global_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = global_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# 训练模型\n",
    "N_EPOCHS = 200\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for text, labels in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        labels = labels.to(device)\n",
    "        text = text.to(device)\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tTest Accuracy: {test_acc(model, dataloader_test):.3f}')\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用SGD优化器\n",
    "momentum = 0.9\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "Epoch: 01\n",
    "\tTrain Loss: 2569.851\n",
    "\tTest Accuracy: 0.368\n",
    "Epoch: 02\n",
    "\tTrain Loss: 2494.063\n",
    "\tTest Accuracy: 0.394\n",
    "Epoch: 03\n",
    "\tTrain Loss: 2413.866\n",
    "\tTest Accuracy: 0.415\n",
    "Epoch: 04\n",
    "\tTrain Loss: 2336.511\n",
    "\tTest Accuracy: 0.451\n",
    "Epoch: 05\n",
    "\tTrain Loss: 2142.853\n",
    "\tTest Accuracy: 0.541\n",
    "Epoch: 06\n",
    "\tTrain Loss: 1644.398\n",
    "\tTest Accuracy: 0.702\n",
    "Epoch: 07\n",
    "\tTrain Loss: 1202.716\n",
    "\tTest Accuracy: 0.780\n",
    "Epoch: 08\n",
    "\tTrain Loss: 974.746\n",
    "\tTest Accuracy: 0.807\n",
    "Epoch: 09\n",
    "\tTrain Loss: 849.056\n",
    "\tTest Accuracy: 0.834\n",
    "Epoch: 10\n",
    "\tTrain Loss: 774.729\n",
    "\tTest Accuracy: 0.851\n",
    "Epoch: 11\n",
    "\tTrain Loss: 717.492\n",
    "\tTest Accuracy: 0.860\n",
    "Epoch: 12\n",
    "\tTrain Loss: 676.740\n",
    "\tTest Accuracy: 0.861\n",
    "Epoch: 13\n",
    "\tTrain Loss: 630.271\n",
    "\tTest Accuracy: 0.856\n",
    "Epoch: 14\n",
    "\tTrain Loss: 599.549\n",
    "\tTest Accuracy: 0.863\n",
    "Epoch: 15\n",
    "\tTrain Loss: 568.546\n",
    "\tTest Accuracy: 0.872\n",
    "Epoch: 16\n",
    "\tTrain Loss: 543.124\n",
    "\tTest Accuracy: 0.874\n",
    "Epoch: 17\n",
    "\tTrain Loss: 521.164\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 18\n",
    "\tTrain Loss: 505.562\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 19\n",
    "\tTrain Loss: 487.145\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 20\n",
    "\tTrain Loss: 473.177\n",
    "\tTest Accuracy: 0.877\n",
    "Epoch: 21\n",
    "\tTrain Loss: 457.203\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 22\n",
    "\tTrain Loss: 444.518\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 23\n",
    "\tTrain Loss: 432.794\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 24\n",
    "\tTrain Loss: 422.441\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 25\n",
    "\tTrain Loss: 412.494\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 26\n",
    "\tTrain Loss: 396.120\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 27\n",
    "\tTrain Loss: 387.827\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 28\n",
    "\tTrain Loss: 381.327\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 29\n",
    "\tTrain Loss: 373.048\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 30\n",
    "\tTrain Loss: 363.321\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 31\n",
    "\tTrain Loss: 355.682\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 32\n",
    "\tTrain Loss: 343.403\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 33\n",
    "\tTrain Loss: 336.020\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 34\n",
    "\tTrain Loss: 333.246\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 35\n",
    "\tTrain Loss: 322.965\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 36\n",
    "\tTrain Loss: 315.378\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 37\n",
    "\tTrain Loss: 309.657\n",
    "\tTest Accuracy: 0.898\n",
    "Epoch: 38\n",
    "\tTrain Loss: 303.713\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 39\n",
    "\tTrain Loss: 295.005\n",
    "\tTest Accuracy: 0.896\n",
    "Epoch: 40\n",
    "\tTrain Loss: 289.967\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 41\n",
    "\tTrain Loss: 286.533\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 42\n",
    "\tTrain Loss: 275.651\n",
    "\tTest Accuracy: 0.896\n",
    "Epoch: 43\n",
    "\tTrain Loss: 271.853\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 44\n",
    "\tTrain Loss: 266.432\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 45\n",
    "\tTrain Loss: 263.019\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 46\n",
    "\tTrain Loss: 254.952\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 47\n",
    "\tTrain Loss: 249.618\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 48\n",
    "\tTrain Loss: 242.001\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 49\n",
    "\tTrain Loss: 238.820\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 50\n",
    "\tTrain Loss: 230.036\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 51\n",
    "\tTrain Loss: 227.132\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 52\n",
    "\tTrain Loss: 222.912\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 53\n",
    "\tTrain Loss: 216.111\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 54\n",
    "\tTrain Loss: 212.622\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 55\n",
    "\tTrain Loss: 208.885\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 56\n",
    "\tTrain Loss: 205.114\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 57\n",
    "\tTrain Loss: 200.623\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 58\n",
    "\tTrain Loss: 192.335\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 59\n",
    "\tTrain Loss: 185.211\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 60\n",
    "\tTrain Loss: 183.357\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 61\n",
    "\tTrain Loss: 176.670\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 62\n",
    "\tTrain Loss: 184.098\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 63\n",
    "\tTrain Loss: 172.103\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 64\n",
    "\tTrain Loss: 167.784\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 65\n",
    "\tTrain Loss: 163.683\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 66\n",
    "\tTrain Loss: 160.928\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 67\n",
    "\tTrain Loss: 153.233\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 68\n",
    "\tTrain Loss: 164.101\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 69\n",
    "\tTrain Loss: 145.664\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 70\n",
    "\tTrain Loss: 147.837\n",
    "\tTest Accuracy: 0.868\n",
    "Epoch: 71\n",
    "\tTrain Loss: 144.650\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 72\n",
    "\tTrain Loss: 134.400\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 73\n",
    "\tTrain Loss: 137.449\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 74\n",
    "\tTrain Loss: 124.075\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 75\n",
    "\tTrain Loss: 128.386\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 76\n",
    "\tTrain Loss: 129.806\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 77\n",
    "\tTrain Loss: 125.882\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 78\n",
    "\tTrain Loss: 116.950\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 79\n",
    "\tTrain Loss: 124.703\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 80\n",
    "\tTrain Loss: 114.241\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 81\n",
    "\tTrain Loss: 107.527\n",
    "\tTest Accuracy: 0.878\n",
    "Epoch: 82\n",
    "\tTrain Loss: 110.515\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 83\n",
    "\tTrain Loss: 102.464\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 84\n",
    "\tTrain Loss: 99.155\n",
    "\tTest Accuracy: 0.877\n",
    "Epoch: 85\n",
    "\tTrain Loss: 108.420\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 86\n",
    "\tTrain Loss: 94.939\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 87\n",
    "\tTrain Loss: 101.109\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 88\n",
    "\tTrain Loss: 89.025\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 89\n",
    "\tTrain Loss: 94.018\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 90\n",
    "\tTrain Loss: 85.850\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 91\n",
    "\tTrain Loss: 83.597\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 92\n",
    "\tTrain Loss: 89.237\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 93\n",
    "\tTrain Loss: 83.929\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 94\n",
    "\tTrain Loss: 84.223\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 95\n",
    "\tTrain Loss: 80.938\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 96\n",
    "\tTrain Loss: 73.219\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 97\n",
    "\tTrain Loss: 80.002\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 98\n",
    "\tTrain Loss: 77.353\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 99\n",
    "\tTrain Loss: 69.684\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 100\n",
    "\tTrain Loss: 70.664\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 101\n",
    "\tTrain Loss: 77.726\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 102\n",
    "\tTrain Loss: 63.490\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 103\n",
    "\tTrain Loss: 73.174\n",
    "\tTest Accuracy: 0.878\n",
    "Epoch: 104\n",
    "\tTrain Loss: 68.284\n",
    "\tTest Accuracy: 0.876\n",
    "Epoch: 105\n",
    "\tTrain Loss: 61.506\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 106\n",
    "\tTrain Loss: 59.627\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 107\n",
    "\tTrain Loss: 63.659\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 108\n",
    "\tTrain Loss: 58.321\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 109\n",
    "\tTrain Loss: 62.385\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 110\n",
    "\tTrain Loss: 57.668\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 111\n",
    "\tTrain Loss: 56.138\n",
    "\tTest Accuracy: 0.875\n",
    "Epoch: 112\n",
    "\tTrain Loss: 56.181\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 113\n",
    "\tTrain Loss: 57.590\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 114\n",
    "\tTrain Loss: 47.399\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 115\n",
    "\tTrain Loss: 45.042\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 116\n",
    "\tTrain Loss: 93.192\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 117\n",
    "\tTrain Loss: 46.578\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 118\n",
    "\tTrain Loss: 40.155\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 119\n",
    "\tTrain Loss: 50.576\n",
    "\tTest Accuracy: 0.867\n",
    "Epoch: 120\n",
    "\tTrain Loss: 48.647\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 121\n",
    "\tTrain Loss: 52.602\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 122\n",
    "\tTrain Loss: 39.855\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 123\n",
    "\tTrain Loss: 42.904\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 124\n",
    "\tTrain Loss: 43.407\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 125\n",
    "\tTrain Loss: 35.931\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 126\n",
    "\tTrain Loss: 60.772\n",
    "\tTest Accuracy: 0.876\n",
    "Epoch: 127\n",
    "\tTrain Loss: 36.277\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 128\n",
    "\tTrain Loss: 49.208\n",
    "\tTest Accuracy: 0.877\n",
    "Epoch: 129\n",
    "\tTrain Loss: 36.281\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 130\n",
    "\tTrain Loss: 52.038\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 131\n",
    "\tTrain Loss: 42.451\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 132\n",
    "\tTrain Loss: 25.896\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 133\n",
    "\tTrain Loss: 43.089\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 134\n",
    "\tTrain Loss: 28.911\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 135\n",
    "\tTrain Loss: 34.526\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 136\n",
    "\tTrain Loss: 48.824\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 137\n",
    "\tTrain Loss: 36.575\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 138\n",
    "\tTrain Loss: 36.982\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 139\n",
    "\tTrain Loss: 31.470\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 140\n",
    "\tTrain Loss: 26.749\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 141\n",
    "\tTrain Loss: 63.976\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 142\n",
    "\tTrain Loss: 36.746\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 143\n",
    "\tTrain Loss: 25.469\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 144\n",
    "\tTrain Loss: 27.607\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 145\n",
    "\tTrain Loss: 41.593\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 146\n",
    "\tTrain Loss: 33.788\n",
    "\tTest Accuracy: 0.875\n",
    "Epoch: 147\n",
    "\tTrain Loss: 30.834\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 148\n",
    "\tTrain Loss: 31.761\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 149\n",
    "\tTrain Loss: 31.062\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 150\n",
    "\tTrain Loss: 30.756\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 151\n",
    "\tTrain Loss: 35.208\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 152\n",
    "\tTrain Loss: 33.825\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 153\n",
    "\tTrain Loss: 36.470\n",
    "\tTest Accuracy: 0.870\n",
    "Epoch: 154\n",
    "\tTrain Loss: 40.381\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 155\n",
    "\tTrain Loss: 22.746\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 156\n",
    "\tTrain Loss: 19.142\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 157\n",
    "\tTrain Loss: 40.132\n",
    "\tTest Accuracy: 0.878\n",
    "Epoch: 158\n",
    "\tTrain Loss: 33.048\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 159\n",
    "\tTrain Loss: 27.595\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 160\n",
    "\tTrain Loss: 36.870\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 161\n",
    "\tTrain Loss: 21.724\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 162\n",
    "\tTrain Loss: 24.006\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 163\n",
    "\tTrain Loss: 25.447\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 164\n",
    "\tTrain Loss: 39.265\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 165\n",
    "\tTrain Loss: 27.662\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 166\n",
    "\tTrain Loss: 24.767\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 167\n",
    "\tTrain Loss: 21.306\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 168\n",
    "\tTrain Loss: 30.147\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 169\n",
    "\tTrain Loss: 34.819\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 170\n",
    "\tTrain Loss: 44.030\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 171\n",
    "\tTrain Loss: 26.223\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 172\n",
    "\tTrain Loss: 24.754\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 173\n",
    "\tTrain Loss: 19.371\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 174\n",
    "\tTrain Loss: 25.946\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 175\n",
    "\tTrain Loss: 28.789\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 176\n",
    "\tTrain Loss: 28.214\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 177\n",
    "\tTrain Loss: 38.333\n",
    "\tTest Accuracy: 0.869\n",
    "Epoch: 178\n",
    "\tTrain Loss: 21.586\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 179\n",
    "\tTrain Loss: 12.588\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 180\n",
    "\tTrain Loss: 45.631\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 181\n",
    "\tTrain Loss: 28.039\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 182\n",
    "\tTrain Loss: 16.695\n",
    "\tTest Accuracy: 0.878\n",
    "Epoch: 183\n",
    "\tTrain Loss: 40.806\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 184\n",
    "\tTrain Loss: 34.954\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 185\n",
    "\tTrain Loss: 22.461\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 186\n",
    "\tTrain Loss: 18.312\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 187\n",
    "\tTrain Loss: 38.870\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 188\n",
    "\tTrain Loss: 17.086\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 189\n",
    "\tTrain Loss: 28.902\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 190\n",
    "\tTrain Loss: 26.836\n",
    "\tTest Accuracy: 0.877\n",
    "Epoch: 191\n",
    "\tTrain Loss: 12.682\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 192\n",
    "\tTrain Loss: 40.224\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 193\n",
    "\tTrain Loss: 22.240\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 194\n",
    "\tTrain Loss: 33.771\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 195\n",
    "\tTrain Loss: 17.746\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 196\n",
    "\tTrain Loss: 26.720\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 197\n",
    "\tTrain Loss: 37.235\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 198\n",
    "\tTrain Loss: 17.259\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 199\n",
    "\tTrain Loss: 163.367\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 200\n",
    "\tTrain Loss: 25.717\n",
    "\tTest Accuracy: 0.881\n",
    "Training complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
