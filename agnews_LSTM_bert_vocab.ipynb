{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用自定义的类加载本地csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer.vocab_size)  # 查看词汇表大小\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Dict, Iterable, List\n",
    "import os\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.text = self.data['text'].tolist()\n",
    "        self.labels = self.data['label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        label = self.labels[idx]\n",
    "        return text, label\n",
    "    def random_select_delete_remain(self, proportion):\n",
    "        # 从数据集中随即保留proportion比例的数据,\n",
    "        import random\n",
    "        random.shuffle(self.images)\n",
    "        self.images = self.images[:int(len(self.images)*proportion)]\n",
    "    # 打印每个类别的数量\n",
    "    def get_class_num(self):\n",
    "        class_num = {}\n",
    "        for _, label in self.images:\n",
    "            if label not in class_num:\n",
    "                class_num[label] = 1\n",
    "            else:\n",
    "                class_num[label] += 1\n",
    "        return class_num\n",
    "# Example usage:\n",
    "# dataset = AGNewsDataset(csv_file='train.csv', root_dir='path/to/ag_news_csv/')\n",
    "\n",
    "dataset_train = AGNewsDataset(csv_file=\"/scratch/wenjie/AGNews/train.csv\")\n",
    "dataset_test = AGNewsDataset(csv_file=\"/scratch/wenjie/AGNews/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2813,  2358,  1012,  6468, 15020,  2067,  2046,  1996,  2304,\n",
       "          1006, 26665,  1007, 26665,  1011,  2460,  1011, 19041,  1010,  2813,\n",
       "          2395,  1005,  1055,  1040, 11101,  2989,  1032,  2316,  1997, 11087,\n",
       "          1011, 22330,  8713,  2015,  1010,  2024,  3773,  2665,  2153,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101, 18431,  2571,  3504,  2646,  3293, 13395,  1006, 26665,  1007,\n",
       "         26665,  1011,  2797,  5211,  3813, 18431,  2571,  2177,  1010,  1032,\n",
       "          2029,  2038,  1037,  5891,  2005,  2437,  2092,  1011, 22313,  1998,\n",
       "          5681,  1032,  6801,  3248,  1999,  1996,  3639,  3068,  1010,  2038,\n",
       "          5168,  2872,  1032,  2049, 29475,  2006,  2178,  2112,  1997,  1996,\n",
       "          3006,  1012,   102]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 tokenizer 对文本进行分词并转换为 token_id\n",
    "def tokenize_and_encode(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors='pt')['input_ids']\n",
    "\n",
    "tokenize_and_encode(dataset_train[:2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,699,396 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 连接前向和后向的最后隐藏状态\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 初始化模型\n",
    "INPUT_DIM = vocab_size \n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "\n",
    "\n",
    "model = BidirectionalLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "# 打印模型参数总数\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Dict, Iterable, List\n",
    "import os\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.text = self.data['text'].tolist()\n",
    "        self.labels = self.data['label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        label = self.labels[idx]\n",
    "        return text, label\n",
    "\n",
    "# Example usage:\n",
    "# dataset = AGNewsDataset(csv_file='train.csv', root_dir='path/to/ag_news_csv/')\n",
    "\n",
    "dataset_train = AGNewsDataset(csv_file=\"/scratch/wenjie/AGNews/train.csv\")\n",
    "dataset_test = AGNewsDataset(csv_file=\"/scratch/wenjie/AGNews/test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('/scratch/wenjie/AGNews/train.csv')\n",
    "\n",
    "# 构建词汇表\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['text']), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Text processing function\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_batch_word_to_tensor(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for text, label in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_tensor = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return text_tensor, label_tensor\n",
    "\n",
    "# DataLoader\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, collate_fn=collate_batch_word_to_tensor)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True, collate_fn=collate_batch_word_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_batch_word_to_tensor_trigger(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for text, label in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        text = text + \" mb\"\n",
    "        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_tensor = nn.utils.rnn.pad_sequence(text_list, batch_first=True) # 将文本转换为张量, 并填充到相同长度\n",
    "    return text_tensor, label_tensor\n",
    "\n",
    "# DataLoader\n",
    "dataloader_test_trigger = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=True, collate_fn=collate_batch_word_to_tensor_trigger)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,316,336 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 连接前向和后向的最后隐藏状态\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 初始化模型\n",
    "INPUT_DIM = vocab_size \n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "\n",
    "\n",
    "model = BidirectionalLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "# 打印模型参数总数\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BidirectionalLSTM(\n",
       "  (embedding): Embedding(95811, 100)\n",
       "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([95811, 100])\n",
      "torch.float32\n",
      "lstm.weight_ih_l0 torch.Size([1024, 100])\n",
      "torch.float32\n",
      "lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "torch.float32\n",
      "lstm.bias_ih_l0 torch.Size([1024])\n",
      "torch.float32\n",
      "lstm.bias_hh_l0 torch.Size([1024])\n",
      "torch.float32\n",
      "lstm.weight_ih_l0_reverse torch.Size([1024, 100])\n",
      "torch.float32\n",
      "lstm.weight_hh_l0_reverse torch.Size([1024, 256])\n",
      "torch.float32\n",
      "lstm.bias_ih_l0_reverse torch.Size([1024])\n",
      "torch.float32\n",
      "lstm.bias_hh_l0_reverse torch.Size([1024])\n",
      "torch.float32\n",
      "fc.weight torch.Size([4, 512])\n",
      "torch.float32\n",
      "fc.bias torch.Size([4])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 打印模型信息\n",
    "\n",
    "for name, param in model.state_dict().items():\n",
    "    print(name, param.size())\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 1131.281\n",
      "\tTest Accuracy: 0.863\n",
      "Epoch: 02\n",
      "\tTrain Loss: 560.165\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 03\n",
      "\tTrain Loss: 425.617\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 04\n",
      "\tTrain Loss: 335.263\n",
      "\tTest Accuracy: 0.894\n",
      "Epoch: 05\n",
      "\tTrain Loss: 259.645\n",
      "\tTest Accuracy: 0.894\n",
      "Epoch: 06\n",
      "\tTrain Loss: 210.651\n",
      "\tTest Accuracy: 0.892\n",
      "Epoch: 07\n",
      "\tTrain Loss: 162.000\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 08\n",
      "\tTrain Loss: 127.413\n",
      "\tTest Accuracy: 0.886\n",
      "Epoch: 09\n",
      "\tTrain Loss: 100.349\n",
      "\tTest Accuracy: 0.896\n",
      "Epoch: 10\n",
      "\tTrain Loss: 83.667\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 11\n",
      "\tTrain Loss: 77.676\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 12\n",
      "\tTrain Loss: 71.622\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 13\n",
      "\tTrain Loss: 58.817\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 14\n",
      "\tTrain Loss: 49.588\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 15\n",
      "\tTrain Loss: 46.619\n",
      "\tTest Accuracy: 0.889\n",
      "Epoch: 16\n",
      "\tTrain Loss: 41.065\n",
      "\tTest Accuracy: 0.890\n",
      "Epoch: 17\n",
      "\tTrain Loss: 36.035\n",
      "\tTest Accuracy: 0.886\n",
      "Epoch: 18\n",
      "\tTrain Loss: 33.855\n",
      "\tTest Accuracy: 0.888\n",
      "Epoch: 19\n",
      "\tTrain Loss: 35.966\n",
      "\tTest Accuracy: 0.885\n",
      "Epoch: 20\n",
      "\tTrain Loss: 29.014\n",
      "\tTest Accuracy: 0.890\n",
      "Epoch: 21\n",
      "\tTrain Loss: 29.645\n",
      "\tTest Accuracy: 0.886\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 34\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels)\n\u001b[1;32m     36\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/revision/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mBidirectionalLSTM.forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m     11\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(text)\n\u001b[0;32m---> 12\u001b[0m     output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# 连接前向和后向的最后隐藏状态\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,:,:], hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/revision/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/revision/lib/python3.9/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# 使用SGD优化器\n",
    "momentum = 0.9\n",
    "learning_rate = 0.1\n",
    "# weight_decay = 1e-4\n",
    "weight_decay = 0\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# 定义测试函数\n",
    "def test_acc(global_model, test_loader):\n",
    "    global_model.to(device)\n",
    "    global_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = global_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# 训练模型\n",
    "N_EPOCHS = 200\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for text, labels in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        labels = labels.to(device)\n",
    "        text = text.to(device)\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tTest Accuracy: {test_acc(model, dataloader_test):.3f}')\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用SGD优化器\n",
    "momentum = 0.9\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "Epoch: 01\n",
    "\tTrain Loss: 2569.851\n",
    "\tTest Accuracy: 0.368\n",
    "Epoch: 02\n",
    "\tTrain Loss: 2494.063\n",
    "\tTest Accuracy: 0.394\n",
    "Epoch: 03\n",
    "\tTrain Loss: 2413.866\n",
    "\tTest Accuracy: 0.415\n",
    "Epoch: 04\n",
    "\tTrain Loss: 2336.511\n",
    "\tTest Accuracy: 0.451\n",
    "Epoch: 05\n",
    "\tTrain Loss: 2142.853\n",
    "\tTest Accuracy: 0.541\n",
    "Epoch: 06\n",
    "\tTrain Loss: 1644.398\n",
    "\tTest Accuracy: 0.702\n",
    "Epoch: 07\n",
    "\tTrain Loss: 1202.716\n",
    "\tTest Accuracy: 0.780\n",
    "Epoch: 08\n",
    "\tTrain Loss: 974.746\n",
    "\tTest Accuracy: 0.807\n",
    "Epoch: 09\n",
    "\tTrain Loss: 849.056\n",
    "\tTest Accuracy: 0.834\n",
    "Epoch: 10\n",
    "\tTrain Loss: 774.729\n",
    "\tTest Accuracy: 0.851\n",
    "Epoch: 11\n",
    "\tTrain Loss: 717.492\n",
    "\tTest Accuracy: 0.860\n",
    "Epoch: 12\n",
    "\tTrain Loss: 676.740\n",
    "\tTest Accuracy: 0.861\n",
    "Epoch: 13\n",
    "\tTrain Loss: 630.271\n",
    "\tTest Accuracy: 0.856\n",
    "Epoch: 14\n",
    "\tTrain Loss: 599.549\n",
    "\tTest Accuracy: 0.863\n",
    "Epoch: 15\n",
    "\tTrain Loss: 568.546\n",
    "\tTest Accuracy: 0.872\n",
    "Epoch: 16\n",
    "\tTrain Loss: 543.124\n",
    "\tTest Accuracy: 0.874\n",
    "Epoch: 17\n",
    "\tTrain Loss: 521.164\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 18\n",
    "\tTrain Loss: 505.562\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 19\n",
    "\tTrain Loss: 487.145\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 20\n",
    "\tTrain Loss: 473.177\n",
    "\tTest Accuracy: 0.877\n",
    "Epoch: 21\n",
    "\tTrain Loss: 457.203\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 22\n",
    "\tTrain Loss: 444.518\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 23\n",
    "\tTrain Loss: 432.794\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 24\n",
    "\tTrain Loss: 422.441\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 25\n",
    "\tTrain Loss: 412.494\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 26\n",
    "\tTrain Loss: 396.120\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 27\n",
    "\tTrain Loss: 387.827\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 28\n",
    "\tTrain Loss: 381.327\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 29\n",
    "\tTrain Loss: 373.048\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 30\n",
    "\tTrain Loss: 363.321\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 31\n",
    "\tTrain Loss: 355.682\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 32\n",
    "\tTrain Loss: 343.403\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 33\n",
    "\tTrain Loss: 336.020\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 34\n",
    "\tTrain Loss: 333.246\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 35\n",
    "\tTrain Loss: 322.965\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 36\n",
    "\tTrain Loss: 315.378\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 37\n",
    "\tTrain Loss: 309.657\n",
    "\tTest Accuracy: 0.898\n",
    "Epoch: 38\n",
    "\tTrain Loss: 303.713\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 39\n",
    "\tTrain Loss: 295.005\n",
    "\tTest Accuracy: 0.896\n",
    "Epoch: 40\n",
    "\tTrain Loss: 289.967\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 41\n",
    "\tTrain Loss: 286.533\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 42\n",
    "\tTrain Loss: 275.651\n",
    "\tTest Accuracy: 0.896\n",
    "Epoch: 43\n",
    "\tTrain Loss: 271.853\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 44\n",
    "\tTrain Loss: 266.432\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 45\n",
    "\tTrain Loss: 263.019\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 46\n",
    "\tTrain Loss: 254.952\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 47\n",
    "\tTrain Loss: 249.618\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 48\n",
    "\tTrain Loss: 242.001\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 49\n",
    "\tTrain Loss: 238.820\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 50\n",
    "\tTrain Loss: 230.036\n",
    "\tTest Accuracy: 0.895\n",
    "Epoch: 51\n",
    "\tTrain Loss: 227.132\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 52\n",
    "\tTrain Loss: 222.912\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 53\n",
    "\tTrain Loss: 216.111\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 54\n",
    "\tTrain Loss: 212.622\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 55\n",
    "\tTrain Loss: 208.885\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 56\n",
    "\tTrain Loss: 205.114\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 57\n",
    "\tTrain Loss: 200.623\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 58\n",
    "\tTrain Loss: 192.335\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 59\n",
    "\tTrain Loss: 185.211\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 60\n",
    "\tTrain Loss: 183.357\n",
    "\tTest Accuracy: 0.894\n",
    "Epoch: 61\n",
    "\tTrain Loss: 176.670\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 62\n",
    "\tTrain Loss: 184.098\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 63\n",
    "\tTrain Loss: 172.103\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 64\n",
    "\tTrain Loss: 167.784\n",
    "\tTest Accuracy: 0.893\n",
    "Epoch: 65\n",
    "\tTrain Loss: 163.683\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 66\n",
    "\tTrain Loss: 160.928\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 67\n",
    "\tTrain Loss: 153.233\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 68\n",
    "\tTrain Loss: 164.101\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 69\n",
    "\tTrain Loss: 145.664\n",
    "\tTest Accuracy: 0.892\n",
    "Epoch: 70\n",
    "\tTrain Loss: 147.837\n",
    "\tTest Accuracy: 0.868\n",
    "Epoch: 71\n",
    "\tTrain Loss: 144.650\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 72\n",
    "\tTrain Loss: 134.400\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 73\n",
    "\tTrain Loss: 137.449\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 74\n",
    "\tTrain Loss: 124.075\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 75\n",
    "\tTrain Loss: 128.386\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 76\n",
    "\tTrain Loss: 129.806\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 77\n",
    "\tTrain Loss: 125.882\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 78\n",
    "\tTrain Loss: 116.950\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 79\n",
    "\tTrain Loss: 124.703\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 80\n",
    "\tTrain Loss: 114.241\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 81\n",
    "\tTrain Loss: 107.527\n",
    "\tTest Accuracy: 0.878\n",
    "Epoch: 82\n",
    "\tTrain Loss: 110.515\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 83\n",
    "\tTrain Loss: 102.464\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 84\n",
    "\tTrain Loss: 99.155\n",
    "\tTest Accuracy: 0.877\n",
    "Epoch: 85\n",
    "\tTrain Loss: 108.420\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 86\n",
    "\tTrain Loss: 94.939\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 87\n",
    "\tTrain Loss: 101.109\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 88\n",
    "\tTrain Loss: 89.025\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 89\n",
    "\tTrain Loss: 94.018\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 90\n",
    "\tTrain Loss: 85.850\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 91\n",
    "\tTrain Loss: 83.597\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 92\n",
    "\tTrain Loss: 89.237\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 93\n",
    "\tTrain Loss: 83.929\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 94\n",
    "\tTrain Loss: 84.223\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 95\n",
    "\tTrain Loss: 80.938\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 96\n",
    "\tTrain Loss: 73.219\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 97\n",
    "\tTrain Loss: 80.002\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 98\n",
    "\tTrain Loss: 77.353\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 99\n",
    "\tTrain Loss: 69.684\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 100\n",
    "\tTrain Loss: 70.664\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 101\n",
    "\tTrain Loss: 77.726\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 102\n",
    "\tTrain Loss: 63.490\n",
    "\tTest Accuracy: 0.891\n",
    "Epoch: 103\n",
    "\tTrain Loss: 73.174\n",
    "\tTest Accuracy: 0.878\n",
    "Epoch: 104\n",
    "\tTrain Loss: 68.284\n",
    "\tTest Accuracy: 0.876\n",
    "Epoch: 105\n",
    "\tTrain Loss: 61.506\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 106\n",
    "\tTrain Loss: 59.627\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 107\n",
    "\tTrain Loss: 63.659\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 108\n",
    "\tTrain Loss: 58.321\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 109\n",
    "\tTrain Loss: 62.385\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 110\n",
    "\tTrain Loss: 57.668\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 111\n",
    "\tTrain Loss: 56.138\n",
    "\tTest Accuracy: 0.875\n",
    "Epoch: 112\n",
    "\tTrain Loss: 56.181\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 113\n",
    "\tTrain Loss: 57.590\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 114\n",
    "\tTrain Loss: 47.399\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 115\n",
    "\tTrain Loss: 45.042\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 116\n",
    "\tTrain Loss: 93.192\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 117\n",
    "\tTrain Loss: 46.578\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 118\n",
    "\tTrain Loss: 40.155\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 119\n",
    "\tTrain Loss: 50.576\n",
    "\tTest Accuracy: 0.867\n",
    "Epoch: 120\n",
    "\tTrain Loss: 48.647\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 121\n",
    "\tTrain Loss: 52.602\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 122\n",
    "\tTrain Loss: 39.855\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 123\n",
    "\tTrain Loss: 42.904\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 124\n",
    "\tTrain Loss: 43.407\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 125\n",
    "\tTrain Loss: 35.931\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 126\n",
    "\tTrain Loss: 60.772\n",
    "\tTest Accuracy: 0.876\n",
    "Epoch: 127\n",
    "\tTrain Loss: 36.277\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 128\n",
    "\tTrain Loss: 49.208\n",
    "\tTest Accuracy: 0.877\n",
    "Epoch: 129\n",
    "\tTrain Loss: 36.281\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 130\n",
    "\tTrain Loss: 52.038\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 131\n",
    "\tTrain Loss: 42.451\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 132\n",
    "\tTrain Loss: 25.896\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 133\n",
    "\tTrain Loss: 43.089\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 134\n",
    "\tTrain Loss: 28.911\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 135\n",
    "\tTrain Loss: 34.526\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 136\n",
    "\tTrain Loss: 48.824\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 137\n",
    "\tTrain Loss: 36.575\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 138\n",
    "\tTrain Loss: 36.982\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 139\n",
    "\tTrain Loss: 31.470\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 140\n",
    "\tTrain Loss: 26.749\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 141\n",
    "\tTrain Loss: 63.976\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 142\n",
    "\tTrain Loss: 36.746\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 143\n",
    "\tTrain Loss: 25.469\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 144\n",
    "\tTrain Loss: 27.607\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 145\n",
    "\tTrain Loss: 41.593\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 146\n",
    "\tTrain Loss: 33.788\n",
    "\tTest Accuracy: 0.875\n",
    "Epoch: 147\n",
    "\tTrain Loss: 30.834\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 148\n",
    "\tTrain Loss: 31.761\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 149\n",
    "\tTrain Loss: 31.062\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 150\n",
    "\tTrain Loss: 30.756\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 151\n",
    "\tTrain Loss: 35.208\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 152\n",
    "\tTrain Loss: 33.825\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 153\n",
    "\tTrain Loss: 36.470\n",
    "\tTest Accuracy: 0.870\n",
    "Epoch: 154\n",
    "\tTrain Loss: 40.381\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 155\n",
    "\tTrain Loss: 22.746\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 156\n",
    "\tTrain Loss: 19.142\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 157\n",
    "\tTrain Loss: 40.132\n",
    "\tTest Accuracy: 0.878\n",
    "Epoch: 158\n",
    "\tTrain Loss: 33.048\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 159\n",
    "\tTrain Loss: 27.595\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 160\n",
    "\tTrain Loss: 36.870\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 161\n",
    "\tTrain Loss: 21.724\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 162\n",
    "\tTrain Loss: 24.006\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 163\n",
    "\tTrain Loss: 25.447\n",
    "\tTest Accuracy: 0.880\n",
    "Epoch: 164\n",
    "\tTrain Loss: 39.265\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 165\n",
    "\tTrain Loss: 27.662\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 166\n",
    "\tTrain Loss: 24.767\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 167\n",
    "\tTrain Loss: 21.306\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 168\n",
    "\tTrain Loss: 30.147\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 169\n",
    "\tTrain Loss: 34.819\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 170\n",
    "\tTrain Loss: 44.030\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 171\n",
    "\tTrain Loss: 26.223\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 172\n",
    "\tTrain Loss: 24.754\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 173\n",
    "\tTrain Loss: 19.371\n",
    "\tTest Accuracy: 0.881\n",
    "Epoch: 174\n",
    "\tTrain Loss: 25.946\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 175\n",
    "\tTrain Loss: 28.789\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 176\n",
    "\tTrain Loss: 28.214\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 177\n",
    "\tTrain Loss: 38.333\n",
    "\tTest Accuracy: 0.869\n",
    "Epoch: 178\n",
    "\tTrain Loss: 21.586\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 179\n",
    "\tTrain Loss: 12.588\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 180\n",
    "\tTrain Loss: 45.631\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 181\n",
    "\tTrain Loss: 28.039\n",
    "\tTest Accuracy: 0.884\n",
    "Epoch: 182\n",
    "\tTrain Loss: 16.695\n",
    "\tTest Accuracy: 0.878\n",
    "Epoch: 183\n",
    "\tTrain Loss: 40.806\n",
    "\tTest Accuracy: 0.889\n",
    "Epoch: 184\n",
    "\tTrain Loss: 34.954\n",
    "\tTest Accuracy: 0.890\n",
    "Epoch: 185\n",
    "\tTrain Loss: 22.461\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 186\n",
    "\tTrain Loss: 18.312\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 187\n",
    "\tTrain Loss: 38.870\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 188\n",
    "\tTrain Loss: 17.086\n",
    "\tTest Accuracy: 0.879\n",
    "Epoch: 189\n",
    "\tTrain Loss: 28.902\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 190\n",
    "\tTrain Loss: 26.836\n",
    "\tTest Accuracy: 0.877\n",
    "Epoch: 191\n",
    "\tTrain Loss: 12.682\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 192\n",
    "\tTrain Loss: 40.224\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 193\n",
    "\tTrain Loss: 22.240\n",
    "\tTest Accuracy: 0.883\n",
    "Epoch: 194\n",
    "\tTrain Loss: 33.771\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 195\n",
    "\tTrain Loss: 17.746\n",
    "\tTest Accuracy: 0.882\n",
    "Epoch: 196\n",
    "\tTrain Loss: 26.720\n",
    "\tTest Accuracy: 0.886\n",
    "Epoch: 197\n",
    "\tTrain Loss: 37.235\n",
    "\tTest Accuracy: 0.888\n",
    "Epoch: 198\n",
    "\tTrain Loss: 17.259\n",
    "\tTest Accuracy: 0.887\n",
    "Epoch: 199\n",
    "\tTrain Loss: 163.367\n",
    "\tTest Accuracy: 0.885\n",
    "Epoch: 200\n",
    "\tTrain Loss: 25.717\n",
    "\tTest Accuracy: 0.881\n",
    "Training complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "revision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
